{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6dbc86-7b7b-4f8b-8d14-d59dd3778f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training custom model using Pytorch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cccdd3c2-6fa1-45eb-bee5-aa62013191d5",
   "metadata": {},
   "source": [
    "The model is built with custom architecture for implementing inference on MCU.\n",
    "Model CNN-FC with loss function of YoloV1 and Optimizer Adam.\n",
    "Metric for testing model is mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147bb3ed-2338-45aa-8bdc-3bee93927910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (2.5.1.post106)\n",
      "Requirement already satisfied: torchvision in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (0.20.1a0+9f8010e)\n",
      "Requirement already satisfied: torchaudio in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: numpy in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from sympy!=1.13.2,>=1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: ipykernel in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (1.8.10)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (8.30.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: psutil in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/minkescanor/anaconda3/envs/minke/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec minke in /home/minkescanor/snap/code/176/.local/share/jupyter/kernels/minke\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install ipykernel\n",
    "!python -m ipykernel install --user --name=minke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fec0194-8b5d-4356-b1a5-d8a37671d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages in pytorch\n",
    "# Import packages for loading data Images\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79eea0ef-1d41-4f55-ac98-6c5dcfb93c02",
   "metadata": {},
   "source": [
    "Building custom YoloV1 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fe6e03-7b8e-4f53-a3cf-ddbe1b864650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=1):\n",
    "        \"\"\"\n",
    "        S: number of grid cells\n",
    "        B: number of bounding boxes per grid cell\n",
    "        C: number of classes\n",
    "        \"\"\"\n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            \n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            \n",
    "           \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 *58 *58, 32),  # Match the flattened size correctly\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.S * self.S * (self.C + self.B * 5))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).reshape(-1, self.S, self.S, self.C + self.B * 5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a115c8a-0ea2-4a41-bcf8-5b3c4d95ffe4",
   "metadata": {},
   "source": [
    "Building Loss function of YoloV1.\n",
    "Warning: // Because the aim of model is to detect only 1 object, \n",
    "Eliminate Classification loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04214af4-4169-4abc-ad98-10dd179c8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov1_loss(predictions, target, S=7, B=2, C=1, lambda_coord=5, lambda_noobj=0.5):\n",
    "    \"\"\"\n",
    "    Compute the loss function for YOLOv1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure target is a tensor\n",
    "    target = torch.stack(target) if isinstance(target, (list, tuple)) else target\n",
    "    predictions = predictions.reshape(-1, S, S, C + B * 5)\n",
    "    target = target.reshape(-1, S, S, C + B * 5)\n",
    "    \n",
    "    # Separate the predictions\n",
    "    pred_boxes = predictions[..., C:C + B * 4].reshape(-1, S, S, B, 4)  # x, y, w, h\n",
    "    pred_scores = predictions[..., C + B * 4:C + B * 5].reshape(-1, S, S, B)  # confidence score\n",
    "    \n",
    "    target_boxes = target[..., C:C + B * 4].reshape(-1, S, S, B, 4)  # x, y, w, h\n",
    "    target_scores = target[..., C + B * 4:C + B * 5].reshape(-1, S, S, B)  # confidence score\n",
    "    \n",
    "    # Calculate IoU (intersection over union) for each box\n",
    "    ious = torch.zeros_like(pred_scores)  # Placeholder for IoU calculation\n",
    "    \n",
    "    # Loss for coordinate prediction (x, y, w, h)\n",
    "    coord_loss = lambda_coord * torch.sum(\n",
    "        target_scores * (torch.sum((pred_boxes - target_boxes) ** 2, dim=-1))\n",
    "    )\n",
    "    \n",
    "    # Loss for confidence score prediction\n",
    "    obj_loss = torch.sum(target_scores * ((pred_scores - ious) ** 2))\n",
    "    noobj_loss = lambda_noobj * torch.sum((1 - target_scores) * (pred_scores ** 2))\n",
    "    \n",
    "    total_loss = coord_loss + obj_loss + noobj_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "742bdcbf-20b6-416b-8c6a-c5baa3e23062",
   "metadata": {},
   "source": [
    "Calculate mAP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73e0736a-abd9-4759-af09-d6d5d07d4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(model, dataloader, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate mean Average Precision (mAP) for the YOLOv1 model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_detections = []\n",
    "    all_ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Extract predicted boxes and scores\n",
    "            pred_boxes = predictions[..., 1:5]  # x, y, w, h\n",
    "            pred_scores = predictions[..., 0]  # Confidence scores\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                boxes = pred_boxes[i].detach().cpu().numpy()\n",
    "                scores = pred_scores[i].detach().cpu().numpy()\n",
    "                all_detections.append((boxes, scores))\n",
    "                \n",
    "                gt_boxes = targets[i] if isinstance(targets[i], torch.Tensor) else torch.tensor(targets[i])\n",
    "                gt_boxes = gt_boxes[..., 1:5].detach().cpu().numpy()\n",
    "                all_ground_truths.append(gt_boxes)\n",
    "    \n",
    "    mean_ap = 0.0  # Placeholder for mAP calculation logic\n",
    "    \n",
    "    return mean_ap"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1eb14862-8f59-44cf-a5b5-7cf12c9d2eb9",
   "metadata": {},
   "source": [
    "Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e09c5-618c-4c05-aa33-cfb4830c86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, targets\n",
    "    \n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        for phase in [\"train\", \"val\", \"test\"]:\n",
    "            image_dir = os.path.join(root_dir, phase, \"images\")\n",
    "            label_dir = os.path.join(root_dir, phase, \"labels\")\n",
    "            \n",
    "            for filename in os.listdir(image_dir):\n",
    "                if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                    self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                    self.label_paths.append(os.path.join(label_dir, filename.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        target = torch.zeros((7, 7, 11))  # Shape: (S, S, C + B * 5)\n",
    "        label_path = self.label_paths[idx]\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                grid_x = int(x * 7)  # Convert to grid cell\n",
    "                grid_y = int(y * 7)  # Convert to grid cell\n",
    "                \n",
    "                x_offset = x * 7 - grid_x\n",
    "                y_offset = y * 7 - grid_y\n",
    "                \n",
    "                target[grid_y, grid_x, :5] = torch.tensor([1, x_offset, y_offset, width, height])\n",
    "                target[grid_y, grid_x, 5:10] = torch.tensor([1, x_offset, y_offset, width, height])\n",
    "                target[grid_y, grid_x, 10] = class_label\n",
    "        \n",
    "        return image/255 , target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd0b450e-2515-4b12-ab85-d340f5bf877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the YOLOv1 model\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images)\n",
    "            loss = yolov1_loss(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        mean_ap = calculate_mAP(model, dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(dataloader)}, mAP: {mean_ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c071f653-69fd-41bb-a49c-ce505d793f31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/minkescanor/Desktop/WORKPLACE/Hust/AI/Object_Detection/dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m YOLODataset(root_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "root_dir = \"/home/minkescanor/Desktop/WORKPLACE/Hust/AI/Object_Detection/dataset\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = YOLODataset(root_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = YOLOv1(S=7, B=2, C=1)\n",
    "train_model(model, dataloader, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
